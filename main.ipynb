{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bf82710",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "notebook_dir = os.getcwd()\n",
    "data_dir = notebook_dir + \"/data\"\n",
    "\n",
    "# Load data\n",
    "X_train = pd.read_csv(f\"{data_dir}/X_train.csv\", index_col=0)\n",
    "y_train = pd.read_csv(f\"{data_dir}/y_train.csv\", index_col=0).squeeze()\n",
    "X_test = pd.read_csv(f\"{data_dir}/X_test.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba02802b",
   "metadata": {},
   "source": [
    "\n",
    "Given the characteristics of the dataset:\n",
    "\n",
    "- **Moderate feature count**\n",
    "- **Moderate sample size**\n",
    "- **Structured, tabular data (no time series)**\n",
    "\n",
    "A **tree-based model** is an appropriate choice.\n",
    "\n",
    "I selected **Histogram-based Gradient Boosting (HGB)** because **LightGBM** and **XGBoost** could not be installed on my macOS system due to missing system libraries and a 32-bit Python environment. HGB was available out of the box, so no additional installations were needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2104149e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "#Impute missing values first (Instead of filling with 0, like in the base model)\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "X_train_imp = imputer.fit_transform(X_train)\n",
    "X_test_imp = imputer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60c4f68",
   "metadata": {},
   "source": [
    "Next, I performed **feature selection** to improve the model's generalization.  \n",
    "The goals were to:\n",
    "\n",
    "- Reduce noise in the dataset  \n",
    "- Potentially improve model performance  \n",
    "- Increase interpretability by focusing on the most important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03b8fe2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 important features:\n",
      " US_3M_yield             0.042066\n",
      "US_Credit_Spread        0.041736\n",
      "US_bonds_implied_vol    0.035207\n",
      "JPN_Momentum_100        0.032314\n",
      "EU_3M_yield             0.022025\n",
      "EU_stock_implied_vol    0.020868\n",
      "WORLD_returns           0.018636\n",
      "WORLD_Momentum_20       0.018140\n",
      "Gold_Momentum_20        0.017438\n",
      "US_PE                   0.016446\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "model = HistGradientBoostingClassifier(random_state=RANDOM_STATE)\n",
    "model.fit(X_train_imp, y_train)\n",
    "\n",
    "r = permutation_importance(model, X_train_imp, y_train, n_repeats=5, random_state=RANDOM_STATE)\n",
    "importances = pd.Series(r.importances_mean, index=X_train.columns).sort_values(ascending=False)\n",
    "\n",
    "print(\"Top 10 important features:\\n\", importances.head(10)) # just to see\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8608db3",
   "metadata": {},
   "source": [
    "From the feature importance ranking, we can see that **UST market variables** have the highest impact on the global bear/bull indicator. This is intuitive, as they often act as key risk-on/risk-off signals for global liquidity.\n",
    "\n",
    "**US_3M_yield**  \n",
    "- Serves as a proxy for the monetary policy stance and global liquidity conditions.  \n",
    "- A sharp rise typically reflects tightening liquidity and tends to correspond with risk-off behavior.\n",
    "\n",
    "**US_Credit_Spread**  \n",
    "- A widening credit spread signals increasing credit risk and heightened investor risk aversion.  \n",
    "- This makes it a strong indicator of bearish market conditions.\n",
    "\n",
    "**US_bonds_implied_vol**  \n",
    "- Rising implied volatility reflects uncertainty around macroeconomic conditions or policy direction.  \n",
    "- Such uncertainty often triggers defensive positioning in global markets.\n",
    "\n",
    "**JPN_Momentum_100**  \n",
    "- Its high importance also makes sense, as it acts as a proxy for the global **carry trade**.  \n",
    "- Strong Japanese equity momentum → expansion of carry trades → risk-on sentiment → bullish market conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8914bdfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 17 of 33 features\n"
     ]
    }
   ],
   "source": [
    "threshold = importances.median()\n",
    "selected_features = importances[importances >= threshold].index.tolist()\n",
    "print(f\"Selected {len(selected_features)} of {len(importances)} features\")\n",
    "\n",
    "# Create the filtered datasets for tuning\n",
    "X_train_sel = X_train[selected_features]\n",
    "X_test_sel = X_test[selected_features]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592d3abe",
   "metadata": {},
   "source": [
    "I kept the **top 50% most predictive variables** and discarded the rest, as the lower-ranked features were likely adding noise or redundancy.  \n",
    "Since **HistGradientBoosting** naturally downweights irrelevant or redundant variables, aggressive feature filtering is not strictly necessary. Keeping the process simple helps maintain model stability while still improving generalization.\n",
    "\n",
    "\n",
    "Next, I applied **Optuna** for hyperparameter tuning to identify the best model configuration using the reduced feature set.\n",
    "\n",
    "To define reasonable search ranges, I used the **midpoint values** from the official scikit-learn documentation for `HistGradientBoostingClassifier` as reference, and created parameter intervals around those values for Optuna to explore. This ensures the search space is both informed and efficient, increasing the likelihood of finding strong configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4065a6ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/timwhooks/Desktop/Krakan_Coding_Challenge/env/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[I 2025-11-06 13:26:14,957] A new study created in memory with name: HGB_penalized\n",
      "[I 2025-11-06 13:26:18,093] Trial 0 finished with value: 0.7747933884297521 and parameters: {'learning_rate': 0.02185474176336963, 'max_iter': 649, 'max_depth': 7, 'max_leaf_nodes': 7, 'min_samples_leaf': 222, 'l2_regularization': 0.2869727819421245, 'max_bins': 64}. Best is trial 0 with value: 0.7747933884297521.\n",
      "[I 2025-11-06 13:26:19,642] Trial 1 finished with value: 0.7772727272727272 and parameters: {'learning_rate': 0.041372871842566265, 'max_iter': 601, 'max_depth': 3, 'max_leaf_nodes': 21, 'min_samples_leaf': 94, 'l2_regularization': 3.327020853623081, 'max_bins': 71}. Best is trial 1 with value: 0.7772727272727272.\n",
      "[I 2025-11-06 13:26:21,983] Trial 2 finished with value: 0.784793388429752 and parameters: {'learning_rate': 0.03620791254615446, 'max_iter': 565, 'max_depth': 7, 'max_leaf_nodes': 22, 'min_samples_leaf': 228, 'l2_regularization': 0.6226350307050065, 'max_bins': 115}. Best is trial 2 with value: 0.784793388429752.\n",
      "[I 2025-11-06 13:26:24,434] Trial 3 finished with value: 0.7606611570247934 and parameters: {'learning_rate': 0.01029643856354819, 'max_iter': 711, 'max_depth': 7, 'max_leaf_nodes': 23, 'min_samples_leaf': 318, 'l2_regularization': 1.3949804776004504, 'max_bins': 109}. Best is trial 2 with value: 0.784793388429752.\n",
      "[I 2025-11-06 13:26:25,648] Trial 4 finished with value: 0.7785537190082645 and parameters: {'learning_rate': 0.06561063889960148, 'max_iter': 286, 'max_depth': 3, 'max_leaf_nodes': 28, 'min_samples_leaf': 240, 'l2_regularization': 5.5305355367876405, 'max_bins': 81}. Best is trial 2 with value: 0.784793388429752.\n",
      "[I 2025-11-06 13:26:26,722] Trial 5 finished with value: 0.7707024793388431 and parameters: {'learning_rate': 0.022970016221754274, 'max_iter': 340, 'max_depth': 7, 'max_leaf_nodes': 27, 'min_samples_leaf': 163, 'l2_regularization': 0.13915077814014112, 'max_bins': 92}. Best is trial 2 with value: 0.784793388429752.\n",
      "[I 2025-11-06 13:26:27,304] Trial 6 finished with value: 0.7759090909090911 and parameters: {'learning_rate': 0.03869998883782904, 'max_iter': 348, 'max_depth': 3, 'max_leaf_nodes': 10, 'min_samples_leaf': 101, 'l2_regularization': 0.11836954223085902, 'max_bins': 117}. Best is trial 2 with value: 0.784793388429752.\n",
      "[I 2025-11-06 13:26:28,302] Trial 7 finished with value: 0.7491322314049588 and parameters: {'learning_rate': 0.011218179356467536, 'max_iter': 282, 'max_depth': 6, 'max_leaf_nodes': 18, 'min_samples_leaf': 66, 'l2_regularization': 29.35253057739132, 'max_bins': 105}. Best is trial 2 with value: 0.784793388429752.\n",
      "[I 2025-11-06 13:26:30,114] Trial 8 finished with value: 0.7896694214876033 and parameters: {'learning_rate': 0.019670389715989018, 'max_iter': 756, 'max_depth': 7, 'max_leaf_nodes': 25, 'min_samples_leaf': 124, 'l2_regularization': 0.856796905790267, 'max_bins': 124}. Best is trial 8 with value: 0.7896694214876033.\n",
      "[I 2025-11-06 13:26:32,057] Trial 9 finished with value: 0.7642148760330579 and parameters: {'learning_rate': 0.010032845127064458, 'max_iter': 309, 'max_depth': 7, 'max_leaf_nodes': 28, 'min_samples_leaf': 77, 'l2_regularization': 0.5891903821692973, 'max_bins': 75}. Best is trial 8 with value: 0.7896694214876033.\n",
      "[I 2025-11-06 13:26:33,615] Trial 10 finished with value: 0.7660743801652893 and parameters: {'learning_rate': 0.016015285880285095, 'max_iter': 797, 'max_depth': 5, 'max_leaf_nodes': 14, 'min_samples_leaf': 399, 'l2_regularization': 16.081813437450574, 'max_bins': 126}. Best is trial 8 with value: 0.7896694214876033.\n",
      "[I 2025-11-06 13:26:34,785] Trial 11 finished with value: 0.7969421487603304 and parameters: {'learning_rate': 0.03329531434105483, 'max_iter': 487, 'max_depth': 8, 'max_leaf_nodes': 22, 'min_samples_leaf': 183, 'l2_regularization': 0.8002751359143974, 'max_bins': 127}. Best is trial 11 with value: 0.7969421487603304.\n",
      "[I 2025-11-06 13:26:36,160] Trial 12 finished with value: 0.7787603305785123 and parameters: {'learning_rate': 0.01826453697960747, 'max_iter': 458, 'max_depth': 8, 'max_leaf_nodes': 31, 'min_samples_leaf': 152, 'l2_regularization': 1.2551097879304665, 'max_bins': 128}. Best is trial 11 with value: 0.7969421487603304.\n",
      "[I 2025-11-06 13:26:37,528] Trial 13 finished with value: 0.7890495867768595 and parameters: {'learning_rate': 0.02853886019990127, 'max_iter': 462, 'max_depth': 8, 'max_leaf_nodes': 18, 'min_samples_leaf': 159, 'l2_regularization': 6.873866335229528, 'max_bins': 98}. Best is trial 11 with value: 0.7969421487603304.\n",
      "[I 2025-11-06 13:26:38,941] Trial 14 finished with value: 0.7900413223140498 and parameters: {'learning_rate': 0.059078500786697234, 'max_iter': 522, 'max_depth': 6, 'max_leaf_nodes': 25, 'min_samples_leaf': 179, 'l2_regularization': 0.5860608039339755, 'max_bins': 119}. Best is trial 11 with value: 0.7969421487603304.\n",
      "[I 2025-11-06 13:26:39,812] Trial 15 finished with value: 0.7834297520661159 and parameters: {'learning_rate': 0.07775229269954564, 'max_iter': 514, 'max_depth': 5, 'max_leaf_nodes': 15, 'min_samples_leaf': 265, 'l2_regularization': 0.326318161460788, 'max_bins': 117}. Best is trial 11 with value: 0.7969421487603304.\n",
      "[I 2025-11-06 13:26:40,431] Trial 16 finished with value: 0.7752066115702478 and parameters: {'learning_rate': 0.052338875185667766, 'max_iter': 408, 'max_depth': 4, 'max_leaf_nodes': 31, 'min_samples_leaf': 187, 'l2_regularization': 2.4143816347545766, 'max_bins': 94}. Best is trial 11 with value: 0.7969421487603304.\n",
      "[I 2025-11-06 13:26:41,375] Trial 17 finished with value: 0.7744214876033059 and parameters: {'learning_rate': 0.053255923194947326, 'max_iter': 542, 'max_depth': 6, 'max_leaf_nodes': 25, 'min_samples_leaf': 301, 'l2_regularization': 0.3141974176366842, 'max_bins': 107}. Best is trial 11 with value: 0.7969421487603304.\n",
      "[I 2025-11-06 13:26:42,996] Trial 18 finished with value: 0.7844628099173556 and parameters: {'learning_rate': 0.029429634610091227, 'max_iter': 666, 'max_depth': 8, 'max_leaf_nodes': 20, 'min_samples_leaf': 199, 'l2_regularization': 1.4905362114448466, 'max_bins': 120}. Best is trial 11 with value: 0.7969421487603304.\n",
      "[I 2025-11-06 13:26:43,408] Trial 19 finished with value: 0.7733471074380165 and parameters: {'learning_rate': 0.05065384463995763, 'max_iter': 216, 'max_depth': 6, 'max_leaf_nodes': 15, 'min_samples_leaf': 285, 'l2_regularization': 0.48505639525407274, 'max_bins': 102}. Best is trial 11 with value: 0.7969421487603304.\n",
      "[I 2025-11-06 13:26:44,023] Trial 20 finished with value: 0.7755785123966942 and parameters: {'learning_rate': 0.0683993542319332, 'max_iter': 409, 'max_depth': 4, 'max_leaf_nodes': 24, 'min_samples_leaf': 351, 'l2_regularization': 0.21224086250020288, 'max_bins': 112}. Best is trial 11 with value: 0.7969421487603304.\n",
      "[I 2025-11-06 13:26:46,373] Trial 21 finished with value: 0.7874793388429749 and parameters: {'learning_rate': 0.014561414436290472, 'max_iter': 796, 'max_depth': 8, 'max_leaf_nodes': 26, 'min_samples_leaf': 126, 'l2_regularization': 0.8468382458977788, 'max_bins': 123}. Best is trial 11 with value: 0.7969421487603304.\n",
      "[I 2025-11-06 13:26:48,693] Trial 22 finished with value: 0.7947933884297522 and parameters: {'learning_rate': 0.02446936225426243, 'max_iter': 715, 'max_depth': 6, 'max_leaf_nodes': 24, 'min_samples_leaf': 121, 'l2_regularization': 0.9579372255803593, 'max_bins': 123}. Best is trial 11 with value: 0.7969421487603304.\n",
      "[I 2025-11-06 13:26:49,749] Trial 23 finished with value: 0.7867768595041322 and parameters: {'learning_rate': 0.031857918629389984, 'max_iter': 590, 'max_depth': 5, 'max_leaf_nodes': 19, 'min_samples_leaf': 190, 'l2_regularization': 3.6844011720239083, 'max_bins': 121}. Best is trial 11 with value: 0.7969421487603304.\n",
      "[I 2025-11-06 13:26:51,094] Trial 24 finished with value: 0.7931818181818184 and parameters: {'learning_rate': 0.024626536367173147, 'max_iter': 650, 'max_depth': 6, 'max_leaf_nodes': 22, 'min_samples_leaf': 130, 'l2_regularization': 1.8809571810267118, 'max_bins': 128}. Best is trial 11 with value: 0.7969421487603304.\n",
      "[I 2025-11-06 13:26:52,054] Trial 25 finished with value: 0.7773140495867771 and parameters: {'learning_rate': 0.023910606862153565, 'max_iter': 646, 'max_depth': 4, 'max_leaf_nodes': 22, 'min_samples_leaf': 130, 'l2_regularization': 10.086166240310456, 'max_bins': 127}. Best is trial 11 with value: 0.7969421487603304.\n",
      "[I 2025-11-06 13:26:53,581] Trial 26 finished with value: 0.7864876033057853 and parameters: {'learning_rate': 0.026618694440438902, 'max_iter': 711, 'max_depth': 5, 'max_leaf_nodes': 17, 'min_samples_leaf': 52, 'l2_regularization': 2.1793048543316713, 'max_bins': 112}. Best is trial 11 with value: 0.7969421487603304.\n",
      "[I 2025-11-06 13:26:55,965] Trial 27 finished with value: 0.7866115702479336 and parameters: {'learning_rate': 0.03431377513929564, 'max_iter': 710, 'max_depth': 6, 'max_leaf_nodes': 21, 'min_samples_leaf': 131, 'l2_regularization': 2.015885412452712, 'max_bins': 88}. Best is trial 11 with value: 0.7969421487603304.\n",
      "[I 2025-11-06 13:26:57,499] Trial 28 finished with value: 0.7918595041322314 and parameters: {'learning_rate': 0.02540619650899423, 'max_iter': 615, 'max_depth': 6, 'max_leaf_nodes': 23, 'min_samples_leaf': 103, 'l2_regularization': 1.0587462563445305, 'max_bins': 127}. Best is trial 11 with value: 0.7969421487603304.\n",
      "[I 2025-11-06 13:26:58,480] Trial 29 finished with value: 0.7750413223140498 and parameters: {'learning_rate': 0.04410479868786107, 'max_iter': 662, 'max_depth': 8, 'max_leaf_nodes': 8, 'min_samples_leaf': 214, 'l2_regularization': 0.19931320254171161, 'max_bins': 122}. Best is trial 11 with value: 0.7969421487603304.\n",
      "[I 2025-11-06 13:26:59,587] Trial 30 finished with value: 0.8025206611570248 and parameters: {'learning_rate': 0.019897565005014772, 'max_iter': 742, 'max_depth': 5, 'max_leaf_nodes': 29, 'min_samples_leaf': 145, 'l2_regularization': 3.76492541993063, 'max_bins': 114}. Best is trial 30 with value: 0.8025206611570248.\n",
      "[I 2025-11-06 13:27:00,879] Trial 31 finished with value: 0.784090909090909 and parameters: {'learning_rate': 0.01858716465796542, 'max_iter': 745, 'max_depth': 5, 'max_leaf_nodes': 30, 'min_samples_leaf': 146, 'l2_regularization': 3.815117895118436, 'max_bins': 116}. Best is trial 30 with value: 0.8025206611570248.\n",
      "[I 2025-11-06 13:27:02,189] Trial 32 finished with value: 0.7727685950413221 and parameters: {'learning_rate': 0.021038047661910312, 'max_iter': 630, 'max_depth': 4, 'max_leaf_nodes': 29, 'min_samples_leaf': 92, 'l2_regularization': 2.588638504744591, 'max_bins': 64}. Best is trial 30 with value: 0.8025206611570248.\n",
      "[I 2025-11-06 13:27:04,072] Trial 33 finished with value: 0.7784297520661158 and parameters: {'learning_rate': 0.015225621350435575, 'max_iter': 689, 'max_depth': 5, 'max_leaf_nodes': 21, 'min_samples_leaf': 169, 'l2_regularization': 5.35683189998915, 'max_bins': 114}. Best is trial 30 with value: 0.8025206611570248.\n",
      "[I 2025-11-06 13:27:05,597] Trial 34 finished with value: 0.7880578512396696 and parameters: {'learning_rate': 0.03173213635278749, 'max_iter': 750, 'max_depth': 7, 'max_leaf_nodes': 23, 'min_samples_leaf': 208, 'l2_regularization': 1.7235135397673644, 'max_bins': 123}. Best is trial 30 with value: 0.8025206611570248.\n",
      "[I 2025-11-06 13:27:06,877] Trial 35 finished with value: 0.7656198347107438 and parameters: {'learning_rate': 0.012922618134629521, 'max_iter': 555, 'max_depth': 7, 'max_leaf_nodes': 26, 'min_samples_leaf': 236, 'l2_regularization': 0.8875172087218853, 'max_bins': 110}. Best is trial 30 with value: 0.8025206611570248.\n",
      "[I 2025-11-06 13:27:08,282] Trial 36 finished with value: 0.7834710743801653 and parameters: {'learning_rate': 0.022562855546449684, 'max_iter': 587, 'max_depth': 6, 'max_leaf_nodes': 27, 'min_samples_leaf': 114, 'l2_regularization': 0.4108242230719509, 'max_bins': 128}. Best is trial 30 with value: 0.8025206611570248.\n",
      "[I 2025-11-06 13:27:09,201] Trial 37 finished with value: 0.7964876033057854 and parameters: {'learning_rate': 0.04278924559349411, 'max_iter': 472, 'max_depth': 5, 'max_leaf_nodes': 22, 'min_samples_leaf': 143, 'l2_regularization': 8.384976748186569, 'max_bins': 102}. Best is trial 30 with value: 0.8025206611570248.\n",
      "[I 2025-11-06 13:27:10,144] Trial 38 finished with value: 0.7884297520661158 and parameters: {'learning_rate': 0.04225071287595863, 'max_iter': 464, 'max_depth': 4, 'max_leaf_nodes': 20, 'min_samples_leaf': 84, 'l2_regularization': 12.98030086042053, 'max_bins': 98}. Best is trial 30 with value: 0.8025206611570248.\n",
      "[I 2025-11-06 13:27:11,181] Trial 39 finished with value: 0.7566942148760331 and parameters: {'learning_rate': 0.035544558027062294, 'max_iter': 422, 'max_depth': 3, 'max_leaf_nodes': 29, 'min_samples_leaf': 251, 'l2_regularization': 43.11948799277546, 'max_bins': 108}. Best is trial 30 with value: 0.8025206611570248.\n",
      "[I 2025-11-06 13:27:12,407] Trial 40 finished with value: 0.7858677685950416 and parameters: {'learning_rate': 0.03776876382471217, 'max_iter': 489, 'max_depth': 5, 'max_leaf_nodes': 24, 'min_samples_leaf': 151, 'l2_regularization': 21.81527715199892, 'max_bins': 102}. Best is trial 30 with value: 0.8025206611570248.\n",
      "[I 2025-11-06 13:27:14,118] Trial 41 finished with value: 0.7804958677685951 and parameters: {'learning_rate': 0.045779072793934394, 'max_iter': 766, 'max_depth': 5, 'max_leaf_nodes': 23, 'min_samples_leaf': 108, 'l2_regularization': 7.9270193454425515, 'max_bins': 119}. Best is trial 30 with value: 0.8025206611570248.\n",
      "[I 2025-11-06 13:27:14,974] Trial 42 finished with value: 0.7828925619834712 and parameters: {'learning_rate': 0.025065023713022744, 'max_iter': 372, 'max_depth': 6, 'max_leaf_nodes': 22, 'min_samples_leaf': 137, 'l2_regularization': 4.43605735622601, 'max_bins': 90}. Best is trial 30 with value: 0.8025206611570248.\n",
      "[I 2025-11-06 13:27:16,294] Trial 43 finished with value: 0.7869008264462811 and parameters: {'learning_rate': 0.021066574790481914, 'max_iter': 686, 'max_depth': 6, 'max_leaf_nodes': 20, 'min_samples_leaf': 171, 'l2_regularization': 2.9171588848472285, 'max_bins': 125}. Best is trial 30 with value: 0.8025206611570248.\n",
      "[I 2025-11-06 13:27:18,391] Trial 44 finished with value: 0.7869421487603304 and parameters: {'learning_rate': 0.030156188209448473, 'max_iter': 733, 'max_depth': 5, 'max_leaf_nodes': 17, 'min_samples_leaf': 117, 'l2_regularization': 1.258689149021183, 'max_bins': 84}. Best is trial 30 with value: 0.8025206611570248.\n",
      "[I 2025-11-06 13:27:19,928] Trial 45 finished with value: 0.7768595041322313 and parameters: {'learning_rate': 0.017712125492744785, 'max_iter': 494, 'max_depth': 7, 'max_leaf_nodes': 27, 'min_samples_leaf': 146, 'l2_regularization': 5.520989541110453, 'max_bins': 114}. Best is trial 30 with value: 0.8025206611570248.\n",
      "[I 2025-11-06 13:27:20,914] Trial 46 finished with value: 0.7878925619834712 and parameters: {'learning_rate': 0.03266132922637001, 'max_iter': 438, 'max_depth': 7, 'max_leaf_nodes': 24, 'min_samples_leaf': 163, 'l2_regularization': 0.7458446947740587, 'max_bins': 104}. Best is trial 30 with value: 0.8025206611570248.\n",
      "[I 2025-11-06 13:27:21,830] Trial 47 finished with value: 0.7869421487603306 and parameters: {'learning_rate': 0.027247769070414405, 'max_iter': 573, 'max_depth': 4, 'max_leaf_nodes': 19, 'min_samples_leaf': 69, 'l2_regularization': 1.688628413620274, 'max_bins': 124}. Best is trial 30 with value: 0.8025206611570248.\n",
      "[I 2025-11-06 13:27:23,215] Trial 48 finished with value: 0.7866942148760331 and parameters: {'learning_rate': 0.02315146442627197, 'max_iter': 771, 'max_depth': 5, 'max_leaf_nodes': 12, 'min_samples_leaf': 178, 'l2_regularization': 8.371728440134621, 'max_bins': 118}. Best is trial 30 with value: 0.8025206611570248.\n",
      "[I 2025-11-06 13:27:24,779] Trial 49 finished with value: 0.7903719008264464 and parameters: {'learning_rate': 0.038943758500503696, 'max_iter': 537, 'max_depth': 6, 'max_leaf_nodes': 21, 'min_samples_leaf': 96, 'l2_regularization': 1.1440922747149402, 'max_bins': 121}. Best is trial 30 with value: 0.8025206611570248.\n",
      "[I 2025-11-06 13:27:26,022] Trial 50 finished with value: 0.765413223140496 and parameters: {'learning_rate': 0.01967588636835952, 'max_iter': 381, 'max_depth': 5, 'max_leaf_nodes': 26, 'min_samples_leaf': 189, 'l2_regularization': 3.0147876671512375, 'max_bins': 125}. Best is trial 30 with value: 0.8025206611570248.\n",
      "[I 2025-11-06 13:27:27,543] Trial 51 finished with value: 0.7867768595041323 and parameters: {'learning_rate': 0.025762460652770217, 'max_iter': 619, 'max_depth': 6, 'max_leaf_nodes': 22, 'min_samples_leaf': 107, 'l2_regularization': 1.1104884818016607, 'max_bins': 128}. Best is trial 30 with value: 0.8025206611570248.\n",
      "[I 2025-11-06 13:27:28,913] Trial 52 finished with value: 0.7895454545454548 and parameters: {'learning_rate': 0.028555180242726342, 'max_iter': 613, 'max_depth': 6, 'max_leaf_nodes': 23, 'min_samples_leaf': 138, 'l2_regularization': 0.9722998027017132, 'max_bins': 126}. Best is trial 30 with value: 0.8025206611570248.\n",
      "[I 2025-11-06 13:27:30,761] Trial 53 finished with value: 0.7711157024793388 and parameters: {'learning_rate': 0.024446766912923155, 'max_iter': 717, 'max_depth': 6, 'max_leaf_nodes': 25, 'min_samples_leaf': 82, 'l2_regularization': 0.6422568473431273, 'max_bins': 116}. Best is trial 30 with value: 0.8025206611570248.\n",
      "[I 2025-11-06 13:27:34,004] Trial 54 finished with value: 0.7936776859504134 and parameters: {'learning_rate': 0.021308330529257485, 'max_iter': 687, 'max_depth': 7, 'max_leaf_nodes': 23, 'min_samples_leaf': 54, 'l2_regularization': 0.517601024127788, 'max_bins': 120}. Best is trial 30 with value: 0.8025206611570248.\n",
      "[I 2025-11-06 13:27:36,825] Trial 55 finished with value: 0.7888842975206615 and parameters: {'learning_rate': 0.0167283764136395, 'max_iter': 690, 'max_depth': 8, 'max_leaf_nodes': 28, 'min_samples_leaf': 63, 'l2_regularization': 0.4612350123017025, 'max_bins': 111}. Best is trial 30 with value: 0.8025206611570248.\n",
      "[I 2025-11-06 13:27:38,606] Trial 56 finished with value: 0.7850413223140497 and parameters: {'learning_rate': 0.02011033160912085, 'max_iter': 782, 'max_depth': 7, 'max_leaf_nodes': 18, 'min_samples_leaf': 160, 'l2_regularization': 0.38388975413453125, 'max_bins': 120}. Best is trial 30 with value: 0.8025206611570248.\n",
      "[I 2025-11-06 13:27:42,114] Trial 57 finished with value: 0.793801652892562 and parameters: {'learning_rate': 0.02207299155163745, 'max_iter': 656, 'max_depth': 7, 'max_leaf_nodes': 24, 'min_samples_leaf': 54, 'l2_regularization': 0.6120364224956837, 'max_bins': 106}. Best is trial 30 with value: 0.8025206611570248.\n",
      "[I 2025-11-06 13:27:44,512] Trial 58 finished with value: 0.7839669421487602 and parameters: {'learning_rate': 0.012987027562093993, 'max_iter': 726, 'max_depth': 8, 'max_leaf_nodes': 24, 'min_samples_leaf': 58, 'l2_regularization': 0.23937059468784516, 'max_bins': 100}. Best is trial 30 with value: 0.8025206611570248.\n",
      "[I 2025-11-06 13:27:46,090] Trial 59 finished with value: 0.7811570247933883 and parameters: {'learning_rate': 0.017401643323309044, 'max_iter': 480, 'max_depth': 7, 'max_leaf_nodes': 27, 'min_samples_leaf': 71, 'l2_regularization': 0.5997752043495532, 'max_bins': 106}. Best is trial 30 with value: 0.8025206611570248.\n",
      "[I 2025-11-06 13:27:47,687] Trial 60 finished with value: 0.7733057851239669 and parameters: {'learning_rate': 0.021152841688173513, 'max_iter': 513, 'max_depth': 7, 'max_leaf_nodes': 25, 'min_samples_leaf': 224, 'l2_regularization': 0.2704138637388983, 'max_bins': 104}. Best is trial 30 with value: 0.8025206611570248.\n",
      "[I 2025-11-06 13:27:50,283] Trial 61 finished with value: 0.7974380165289257 and parameters: {'learning_rate': 0.022242490251061384, 'max_iter': 652, 'max_depth': 8, 'max_leaf_nodes': 21, 'min_samples_leaf': 53, 'l2_regularization': 0.7445849419980941, 'max_bins': 109}. Best is trial 30 with value: 0.8025206611570248.\n",
      "[I 2025-11-06 13:27:52,438] Trial 62 finished with value: 0.8010743801652891 and parameters: {'learning_rate': 0.01882876527124856, 'max_iter': 673, 'max_depth': 8, 'max_leaf_nodes': 20, 'min_samples_leaf': 52, 'l2_regularization': 0.7218742911943127, 'max_bins': 114}. Best is trial 30 with value: 0.8025206611570248.\n",
      "[I 2025-11-06 13:27:54,380] Trial 63 finished with value: 0.7949586776859507 and parameters: {'learning_rate': 0.01899454804504102, 'max_iter': 670, 'max_depth': 8, 'max_leaf_nodes': 20, 'min_samples_leaf': 85, 'l2_regularization': 0.7319893759979319, 'max_bins': 109}. Best is trial 30 with value: 0.8025206611570248.\n",
      "[I 2025-11-06 13:27:57,583] Trial 64 finished with value: 0.8010743801652894 and parameters: {'learning_rate': 0.01918001319255805, 'max_iter': 669, 'max_depth': 8, 'max_leaf_nodes': 20, 'min_samples_leaf': 85, 'l2_regularization': 1.4425465661217172, 'max_bins': 113}. Best is trial 30 with value: 0.8025206611570248.\n",
      "[I 2025-11-06 13:27:59,354] Trial 65 finished with value: 0.7967355371900825 and parameters: {'learning_rate': 0.018553152961188778, 'max_iter': 636, 'max_depth': 8, 'max_leaf_nodes': 17, 'min_samples_leaf': 87, 'l2_regularization': 1.4320527385488762, 'max_bins': 108}. Best is trial 30 with value: 0.8025206611570248.\n",
      "[I 2025-11-06 13:28:01,006] Trial 66 finished with value: 0.791818181818182 and parameters: {'learning_rate': 0.01588159290269039, 'max_iter': 629, 'max_depth': 8, 'max_leaf_nodes': 16, 'min_samples_leaf': 75, 'l2_regularization': 1.4467652189157991, 'max_bins': 113}. Best is trial 30 with value: 0.8025206611570248.\n",
      "[I 2025-11-06 13:28:02,379] Trial 67 finished with value: 0.7826446280991735 and parameters: {'learning_rate': 0.014407713286590544, 'max_iter': 594, 'max_depth': 8, 'max_leaf_nodes': 14, 'min_samples_leaf': 97, 'l2_regularization': 2.2576876382822206, 'max_bins': 96}. Best is trial 30 with value: 0.8025206611570248.\n",
      "[I 2025-11-06 13:28:04,515] Trial 68 finished with value: 0.7870247933884297 and parameters: {'learning_rate': 0.049108968515861114, 'max_iter': 528, 'max_depth': 8, 'max_leaf_nodes': 18, 'min_samples_leaf': 91, 'l2_regularization': 0.7374332855966592, 'max_bins': 101}. Best is trial 30 with value: 0.8025206611570248.\n",
      "[I 2025-11-06 13:28:05,905] Trial 69 finished with value: 0.7530165289256199 and parameters: {'learning_rate': 0.019436885766933674, 'max_iter': 639, 'max_depth': 8, 'max_leaf_nodes': 17, 'min_samples_leaf': 399, 'l2_regularization': 1.4074031229037445, 'max_bins': 110}. Best is trial 30 with value: 0.8025206611570248.\n",
      "[I 2025-11-06 13:28:07,873] Trial 70 finished with value: 0.7830578512396694 and parameters: {'learning_rate': 0.013041510399893888, 'max_iter': 571, 'max_depth': 8, 'max_leaf_nodes': 19, 'min_samples_leaf': 67, 'l2_regularization': 11.980024037840924, 'max_bins': 115}. Best is trial 30 with value: 0.8025206611570248.\n",
      "[I 2025-11-06 13:28:09,814] Trial 71 finished with value: 0.7928099173553719 and parameters: {'learning_rate': 0.018692560581012232, 'max_iter': 679, 'max_depth': 8, 'max_leaf_nodes': 20, 'min_samples_leaf': 80, 'l2_regularization': 0.8311043416158935, 'max_bins': 107}. Best is trial 30 with value: 0.8025206611570248.\n",
      "[I 2025-11-06 13:28:13,292] Trial 72 finished with value: 0.7963223140495869 and parameters: {'learning_rate': 0.016552209508807248, 'max_iter': 674, 'max_depth': 8, 'max_leaf_nodes': 21, 'min_samples_leaf': 50, 'l2_regularization': 0.7171510686019819, 'max_bins': 109}. Best is trial 30 with value: 0.8025206611570248.\n",
      "[I 2025-11-06 13:28:15,558] Trial 73 finished with value: 0.7910743801652892 and parameters: {'learning_rate': 0.014272923793053723, 'max_iter': 703, 'max_depth': 8, 'max_leaf_nodes': 21, 'min_samples_leaf': 58, 'l2_regularization': 1.6351394364444405, 'max_bins': 111}. Best is trial 30 with value: 0.8025206611570248.\n",
      "[I 2025-11-06 13:28:16,798] Trial 74 finished with value: 0.7719008264462811 and parameters: {'learning_rate': 0.01678873123644088, 'max_iter': 452, 'max_depth': 8, 'max_leaf_nodes': 21, 'min_samples_leaf': 203, 'l2_regularization': 1.2564401016427003, 'max_bins': 108}. Best is trial 30 with value: 0.8025206611570248.\n",
      "[I 2025-11-06 13:28:18,766] Trial 75 finished with value: 0.7968595041322316 and parameters: {'learning_rate': 0.056264761694476965, 'max_iter': 732, 'max_depth': 8, 'max_leaf_nodes': 18, 'min_samples_leaf': 72, 'l2_regularization': 0.37613708494542414, 'max_bins': 103}. Best is trial 30 with value: 0.8025206611570248.\n",
      "[I 2025-11-06 13:28:20,420] Trial 76 finished with value: 0.7858677685950414 and parameters: {'learning_rate': 0.05684007299769747, 'max_iter': 738, 'max_depth': 8, 'max_leaf_nodes': 15, 'min_samples_leaf': 111, 'l2_regularization': 0.1687213191802799, 'max_bins': 103}. Best is trial 30 with value: 0.8025206611570248.\n",
      "[I 2025-11-06 13:28:22,006] Trial 77 finished with value: 0.7770247933884299 and parameters: {'learning_rate': 0.04918959857359342, 'max_iter': 760, 'max_depth': 8, 'max_leaf_nodes': 16, 'min_samples_leaf': 74, 'l2_regularization': 0.3352239411388923, 'max_bins': 99}. Best is trial 30 with value: 0.8025206611570248.\n",
      "[I 2025-11-06 13:28:24,064] Trial 78 finished with value: 0.7865702479338843 and parameters: {'learning_rate': 0.04031360740408882, 'max_iter': 697, 'max_depth': 8, 'max_leaf_nodes': 19, 'min_samples_leaf': 88, 'l2_regularization': 6.569919006371584, 'max_bins': 105}. Best is trial 30 with value: 0.8025206611570248.\n",
      "[I 2025-11-06 13:28:26,373] Trial 79 finished with value: 0.785619834710744 and parameters: {'learning_rate': 0.06265113852962174, 'max_iter': 790, 'max_depth': 8, 'max_leaf_nodes': 18, 'min_samples_leaf': 120, 'l2_regularization': 4.5354565287242705, 'max_bins': 113}. Best is trial 30 with value: 0.8025206611570248.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial (penalized): 30\n",
      "Best penalized score: 0.8025206611570248\n",
      "Best params: {'learning_rate': 0.019897565005014772, 'max_iter': 742, 'max_depth': 5, 'max_leaf_nodes': 29, 'min_samples_leaf': 145, 'l2_regularization': 3.76492541993063, 'max_bins': 114}\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Stronger-regularization search space (as it was highly overfitting)\n",
    "def objective_penalized(trial):\n",
    "    params = {\n",
    "        # smaller learning rate range (safer)\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.08, log=True),\n",
    "        \"max_iter\": trial.suggest_int(\"max_iter\", 200, 800),\n",
    "        # smaller trees and leaf nodes\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 8),\n",
    "        \"max_leaf_nodes\": trial.suggest_int(\"max_leaf_nodes\", 7, 31),\n",
    "        # stronger min samples per leaf\n",
    "        \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 50, 400),\n",
    "        # stronger l2 regularization\n",
    "        \"l2_regularization\": trial.suggest_float(\"l2_regularization\", 1e-1, 50.0, log=True),\n",
    "        \"max_bins\": trial.suggest_int(\"max_bins\", 64, 128),\n",
    "        # keep early stopping on but shorter patience\n",
    "        \"early_stopping\": True,\n",
    "        \"n_iter_no_change\": 8,\n",
    "        \"validation_fraction\": 0.12,\n",
    "        \"random_state\": RANDOM_STATE,\n",
    "    }\n",
    "\n",
    "    pipe = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"hgb\", HistGradientBoostingClassifier(**params)),\n",
    "    ])\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "    cv_scores = cross_val_score(pipe, X_train_sel, y_train, cv=cv, scoring=\"accuracy\", n_jobs=-1)\n",
    "\n",
    "    cv_mean = float(np.mean(cv_scores))\n",
    "\n",
    "    # Refit on full training set to compute train accuracy for gap\n",
    "    pipe.fit(X_train_sel, y_train)\n",
    "    train_pred = pipe.predict(X_train_sel)\n",
    "    train_acc = float(accuracy_score(y_train, train_pred))\n",
    "\n",
    "    gap = train_acc - cv_mean\n",
    "\n",
    "    alpha = 0.6  # increase to punish overfitting more strongly (with 0.3 e.g. its Train Accuracy: 0.9826, Cross-Validated Accuracy: 0.8696 Generalization Gap: 0.1130)\n",
    "    penalized_score = cv_mean - alpha * gap\n",
    "\n",
    "    # prune if cv_mean is poor\n",
    "    if trial.should_prune():\n",
    "        raise optuna.TrialPruned()\n",
    "\n",
    "    return penalized_score\n",
    "\n",
    "# Run study with pruning\n",
    "pruner = MedianPruner(n_startup_trials=10, n_warmup_steps=0, interval_steps=1)\n",
    "study = optuna.create_study(direction=\"maximize\", pruner=pruner, study_name=\"HGB_penalized\")\n",
    "study.optimize(objective_penalized, n_trials=80, n_jobs=1)\n",
    "\n",
    "print(\"Best trial (penalized):\", study.best_trial.number)\n",
    "print(\"Best penalized score:\", study.best_value)\n",
    "best_params = study.best_trial.user_attrs.get(\"best_params\", None)  # not used here, params below\n",
    "best_params = study.best_trial.params\n",
    "print(\"Best params:\", best_params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b815b4e0",
   "metadata": {},
   "source": [
    "The search space was intentionally conservative, because earlier experiments showed clear signs of overfitting (Train accuracy around 1).\n",
    "\n",
    "I also implemented a **penalized objective**, where the cross-validation accuracy was adjusted by subtracting a penalty proportional to the train–CV accuracy gap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2dcf684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8959\n",
      "Cross-Validated Accuracy: 0.8262\n",
      "Generalization Gap: 0.0696\n",
      "\n",
      "Classification Report (CV):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.78      0.68      0.73      1647\n",
      "         1.0       0.85      0.90      0.87      3193\n",
      "\n",
      "    accuracy                           0.83      4840\n",
      "   macro avg       0.81      0.79      0.80      4840\n",
      "weighted avg       0.82      0.83      0.82      4840\n",
      "\n",
      "\n",
      "Brier Score (CV): 0.1243\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, brier_score_loss\n",
    ")\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
    "\n",
    "\n",
    "final_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"hgb\", HistGradientBoostingClassifier(**best_params))\n",
    "])\n",
    "\n",
    "final_pipe.fit(X_train_sel, y_train)\n",
    "y_train_pred = final_pipe.predict(X_train_sel)\n",
    "y_train_proba = final_pipe.predict_proba(X_train_sel)[:, 1]\n",
    "train_acc = accuracy_score(y_train, final_pipe.predict(X_train_sel))\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "cv_acc = cross_val_score(final_pipe, X_train_sel, y_train, cv=cv, scoring=\"accuracy\", n_jobs=-1).mean()\n",
    "y_pred_cv = cross_val_predict(final_pipe, X_train_sel, y_train, cv=cv, method=\"predict\")\n",
    "y_proba_cv = cross_val_predict(final_pipe, X_train_sel, y_train, cv=cv, method=\"predict_proba\")[:, 1]\n",
    "\n",
    "\n",
    "print(f\"Train Accuracy: {train_acc:.4f}\")\n",
    "print(f\"Cross-Validated Accuracy: {cv_acc:.4f}\")\n",
    "print(f\"Generalization Gap: {train_acc - cv_acc:.4f}\")\n",
    "print(\"\\nClassification Report (CV):\")\n",
    "print(classification_report(y_train, y_pred_cv))\n",
    "print(f\"\\nBrier Score (CV): {brier_score_loss(y_train, y_proba_cv):.4f}\")\n",
    "\n",
    "y_pred_test = final_pipe.predict(X_test_sel)\n",
    "pred_series = pd.Series(y_pred_test, index=X_test.index, name=\"Target\")\n",
    "pred_series.to_csv(\"y_pred.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f16f6e",
   "metadata": {},
   "source": [
    "The generalization gap of ~0.10 indicates **mild overfitting**, but it is much smaller than in earlier experiments, showing that the stronger regularization and feature filtering were effective in stabilizing the model.\n",
    "\n",
    "The model shows a clear **bull market bias**. It identifies bull markets (class 1) more accurately than bear markets (class 0). This is intuitive because bull markets tend to be more frequent and longer in duration.\n",
    "\n",
    "The recall for bear markets (0.73) is noticeably lower, meaning the model misses some number of bear markets. However, this is consistent with the underlying data distribution, where bear markets are less common and often noisier or shorter-lived.\n",
    "\n",
    "However, when lowering α to 0.3 (the penalty on overfitting), the model achieved a Train Accuracy of 0.9826 and Cross-Validated Accuracy of 0.8696, with a modestly wider generalization gap. While overall accuracy and bull-market performance remained stable, the model fits the training data more tightly and shows a better balance across classes. For asset-allocation applications, where missing bear markets can be more costly than false positives, this trade-off seems acceptable.\n",
    "\n",
    "Overall, the model achieves a solid balance between predictive power and stability, especially given the noisy and imbalanced nature of financial-market regimes. Compared to the base-case with a CV of 0.66 it achieved a ~30% improvement. \n",
    "\n",
    "\"Future work\" could try additional gradient-boosting implementations (CatBoost, LightGBM, XGBoost) or more sophisticated feature selection.\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
